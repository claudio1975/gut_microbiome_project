# Config arguments for the project

# Data path
data:
  # Data preprocessing paths
  srs_to_otu_parquet: "data_preprocessing/mapref_data/samples-otus-97.parquet"
  otu_to_dna_parquet: "data_preprocessing/mapref_data/otus_97_to_dna.parquet"
  # Dataset path
  dataset_path: "data_preprocessing/datasets/goldberg/all_groups.csv" # path to preprocessed data
  # Embeddings paths
  dna_csv_dir: "data_preprocessing/dna_sequences"
  dna_embeddings_dir: "data_preprocessing/dna_embeddings"
  microbiome_embeddings_dir: "data_preprocessing/microbiome_embeddings"
  # MicrobiomeTransformer checkpoint path
  mirobiome_transformer_checkpoint: "data/checkpoint_epoch_0_final_epoch3_conf00.pt"
  # ProkBERT model name
  embedding_model: "neuralbioinfo/prokbert-mini-long"
  batch_size_embedding: 6
  device: "cpu"  # cpu, cuda, or mps
  hugging_face:
    download_path: "huggingface_datasets"
    dataset_name: "Tanaka" # e.g., "Diabimmune", "Goldberg", "Gadir"
    base_repo_url: "https://huggingface.co/datasets/hugging-science"
    pull_from_huggingface: true
    csv_filename: "month_2.csv" # e.g., "Month_2.csv" for Diabimmune, "T1.csv" for Goldberg, "gadir_all_months.csv" for Gadir

# model
model:
  classifier: "logreg" # default classifier [logreg, svm, rf, mlp]
  use_scaler: true  # Apply StandardScaler before classifier (recommended for logreg, svm, mlp)

  # Parameter grids for grid search - one per classifier type
  param_grids:
    logreg:
      C: [0.001, 0.01, 0.1, 1, 10]
      penalty: ["l1", "l2"]
      class_weight: ["balanced"]
      solver: ["saga"]  # saga supports both l1 and l2
      max_iter: [1000, 2000]

    svm:
      C: [0.1, 1, 10]
      kernel: ["linear", "rbf"]
      gamma: ["scale", "auto"]
      class_weight: ["balanced"]

    rf:
      n_estimators: [50, 100, 200]
      max_depth: [5, 10, 20, null]
      min_samples_split: [2, 5]
      min_samples_leaf: [1, 2]
      class_weight: ["balanced"]

    mlp:
      hidden_layer_sizes: [[64], [128], [64, 32]]
      activation: ["relu", "tanh"]
      solver: ["adam"]
      alpha: [0.0001, 0.001, 0.01]
      learning_rate: ["constant", "adaptive"]
      max_iter: [500, 1000]

# evaluation
evaluation:
  results_output_dir: "eval_results"
  cv_folds: 5  # number of cross-validation folds for final evaluation
  grid_search_cv_folds: 5  # folds for grid search (inner CV)
  grid_search_scoring: "roc_auc"  # scoring metric for grid search
  grid_search_random_state: 42  # random state for grid search CV
  final_eval_random_state: 123  # DIFFERENT random state for final unbiased evaluation
  save_normalized_cm: true

# Dataset valid filenames
valid_filenames:
  Tanaka:
    - "month_1.csv"
    - "month_2.csv"
    - "month_3.csv"
    - "month_6.csv"
    - "month_12.csv"
    - "month_24.csv"
    - "month_36.csv"
  Diabimmune:
    - "Month_1.csv"
    - "Month_2.csv"
    - "Month_3.csv"
    - "Month_4.csv"
    - "Month_5.csv"
    - "Month_6.csv"
    - "Month_7.csv"
    - "Month_8.csv"
    - "Month_9.csv"
    - "Month_10.csv"
    - "Month_11.csv"
    - "Month_12.csv"
    - "Month_13.csv"
    - "Month_14.csv"
    - "Month_15.csv"
    - "Month_16.csv"
    - "Month_17.csv"
    - "Month_18.csv"
    - "Month_19.csv"
    - "Month_20.csv"
    - "Month_21.csv"
    - "Month_22.csv"
    - "Month_23.csv"
    - "Month_24.csv"
    - "Month_25.csv"
    - "Month_26.csv"
    - "Month_27.csv"
    - "Month_28.csv"
    - "Month_29.csv"
    - "Month_30.csv"
    - "Month_31.csv"
    - "Month_32.csv"
    - "Month_33.csv"
    - "Month_34.csv"
    - "Month_35.csv"
    - "Month_36.csv"
    - "Month_37.csv"
    - "Month_38.csv"
  Goldberg:
    - "T1.csv"
    - "T2.csv"
    - "T3.csv"
  Gadir:
    - "gadir_all_months.csv"
    - "gadir_preprocessed_0-6_months.csv"
    - "gadir_preprocessed_6-12_months.csv"
    - "gadir_preprocessed_12-18_months.csv"
    - "gadir_preprocessed_18-24_months.csv"
    - "gadir_preprocessed_24-30_months.csv"
    - "gadir_preprocessed_30+_months.csv"
